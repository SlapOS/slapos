[buildout]
eggs-directory = {{ buildout['eggs-directory'] }}
develop-eggs-directory = {{ buildout['develop-eggs-directory'] }}

extends =
  {{ monitor_template_cfg }}

parts =
  directory
  monitor-base
  ollama-instance
  ollama-download-model-script
  ollama-run-script
  ollama-model-download-source-config
  ollama-model-download-processed-config
  ollama-model-download-wrapper
  ollama-model-download-processed-config-promise
  ollama-service
  ollama-run-model-service
  request-frontend
  ollama-frontend-promise
  publish-connection-parameter

[slap-configuration]
recipe = slapos.cookbook:slapconfiguration
computer = ${slap-connection:computer-id}
partition = ${slap-connection:partition-id}
url = ${slap-connection:server-url}
key = ${slap-connection:key-file}
cert = ${slap-connection:cert-file}
model = {{ model | default('deepseek-r1:14b') }}

[directory]
recipe = slapos.cookbook:mkdirectory
home = ${buildout:directory}
etc = ${:home}/etc
var = ${:home}/var
srv = ${:home}/srv
bin = ${:home}/bin
tmp = ${:home}/tmp
log = ${:var}/log
services = ${:etc}/service
models = ${:srv}/models
scripts = ${:etc}/run
run = ${:var}/run
status = ${:var}/status
ollama-model-download-expose = ${monitor-directory:private}/ollama-model-download
www = ${:srv}/www
httpd-log = ${:var}/log/apache

[ollama-download-model-script]
recipe = slapos.recipe.template:jinja2
url = {{ download_script_template }}
output = ${directory:bin}/ollama-download-model.py
mode = 0755
context =
  key directory_log directory:log
  key directory_models directory:models
  key ollama_instance_location ollama-instance:location

[ollama-run-script]
recipe = slapos.recipe.template:jinja2
url = {{ ollama_run_template }}
output = ${directory:bin}/ollama-run.py
mode = 0755
context =
  key directory_log directory:log
  key config_file ollama-model-download-source-config:output
  key ollama_instance_location ollama-instance:location

[empty-file-state-base-promise]
<= monitor-promise-base
promise = check_file_state
name = ${:_buildout_section_name_}.py
config-state = empty
config-url = ${monitor-base:base-url}/private/ollama-model-download/${:filename}

[ollama-model-download-processed-config]
recipe = slapos.recipe.build
install =
  import os
  import hashlib

  if not os.path.exists(location):
    os.mkdir(location)

  state_file = '${:state-file}'
  state_dir = os.path.dirname(state_file)
  if not os.path.exists(state_dir):
    os.makedirs(state_dir)

  with open(state_file, 'w') as f:
    try:
      with open('${:config-file}', 'r') as config_handler, open('${:processed-file}') as processed_handler:
        processed = processed_handler.read()
        model_name = config_handler.read()
        if processed == model_name:
            # Model matched, create an empty file
            open(state_file, 'w').close()
        else:
            # Error happened, write error message to the stats file
            # Looks like the promise check has some kind of 'delay'
            # even the processed file changed, the output read from file is still the old one
            # we need to run "slapos node instance" multiple times.
            f.write(f"ERR: processed '{processed}' != desired '{model_name}'\n")
    except Exception as e:
      f.write(str(e))
update = ${:install}
# deepseek-r1:14b
config-file = ${ollama-model-download-source-config:output}
state-filename = ollama-model-download-processed-config.state
# /srv/slapgrid/slappart14/srv/runner/instance/slappart0/srv/monitor/private/ollama-model-download ERR: processed '' != desired 'deepseek-r1:14b'
state-file = ${directory:ollama-model-download-expose}/${:state-filename}
# /srv/slapgrid/slappart14/srv/runner/instance/slappart0/var/run/ollama-model-download-processed empty or model name
processed-file = ${directory:var}/run/ollama-model-download-processed

[ollama-model-download-source-config]
recipe = slapos.recipe.template:jinja2
inline =
  {{ model }}
model = {{ model | default('deepseek-r1:14b') }}
context =
  key model :model
#  deepseek-r1:14b
output = ${directory:etc}/ollama-model-download.conf

[ollama-model-download-wrapper]
# Wrapper to execute ollama model download
recipe = slapos.cookbook:wrapper
wrapper-path = ${directory:scripts}/ollama-model-updater
command-line = ${directory:bin}/ollama-download-model.py ${:config} ${ollama-model-download-processed-config:processed-file} ${:error-state-file}
config = ${ollama-model-download-source-config:output}
error-state-filename = ollama-model-download-error.text
error-state-file = ${directory:ollama-model-download-expose}/${:error-state-filename}
hash-existing-files = ${buildout:directory}/software_release/buildout.cfg

[ollama-model-download-processed-config-promise]
# Promise to check if the model has been downloaded
<= empty-file-state-base-promise
filename = ${ollama-model-download-processed-config:state-filename}
config-filename = ${ollama-model-download-processed-config:state-file}

[ollama-instance]
recipe = plone.recipe.command
pre-command =
  if pgrep -f "ollama serve"; then
    echo "Stopping Ollama service..."
    pkill -f "ollama serve" || true
    rm {{ ollama_path }}/ollama
    sleep 2
  fi
command =
  mkdir -p ${directory:bin}
  cp {{ ollama_path }}/ollama ${directory:bin}/
update-command = ${:command}
stop-on-error = true
location = ${directory:bin}

[ollama-service]
recipe = slapos.cookbook:wrapper
command-line = ${ollama-instance:location}/ollama serve
wrapper-path = ${directory:services}/ollama-service
environment =
  PATH={{ cmake_location }}/bin:{{ git_location }}/bin:{{ gcc_location }}/bin:{{ ninja_location }}/bin:%(PATH)s
  CC={{ gcc_location }}/bin/gcc
  CXX={{ gcc_location }}/bin/g++
  CGO_ENABLED=1
  OLLAMA_LLM_LIBRARY=cpu_avx2

[ollama-run-model-service]
recipe = slapos.cookbook:wrapper
command-line = ${directory:bin}/ollama-run.py ${:config-file} ${ollama-model-download-processed-config:state-file}
wrapper-path = ${directory:services}/ollama-run-model-service
environment =
  OLLAMA_MODELS=${directory:models}
  CUDA_VISIBLE_DEVICES=-1
  OLLAMA_HOST=0.0.0.0:11434
config-file = ${ollama-model-download-source-config:output}
depends =
  ${ollama-model-download-processed-config-promise:recipe}
  ${ollama-instance:recipe}
  ${ollama-model-download-wrapper:recipe}

[request-frontend]
<= slap-connection
recipe = slapos.cookbook:requestoptional
name = Ollama Frontend
# Standard Apache frontend software URL
software-url = http://git.erp5.org/gitweb/slapos.git/blob_plain/HEAD:/software/apache-frontend/software.cfg
software-type = default
shared = true
config-url = http://${slap-configuration:ipv4-random}:11434
return = secure_access domai

[ollama-frontend-promise]
<= monitor-promise-base
promise = check_url_available
name = ollama-http-frontend.py
url = ${request-frontend:connection-secure_access}
config-url = ${:url}

[publish-connection-parameter]
recipe = slapos.cookbook:publish
url = ${request-frontend:connection-secure_access}
ollama-url = http://${slap-configuration:ipv4-random}:11434
model = ${slap-configuration:model}
model-download-log = ${directory:log}/ollama-model-download.log
service-log = ${directory:log}/ollama-service.log
