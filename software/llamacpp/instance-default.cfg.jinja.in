[buildout]
parts =
  huggingface-cli
  directory
  llamacpp-service
  huggingface-cli-download-script
  huggingface-cli-download
  huggingface-download-promise
  llamacpp-server-run-script
  nginx-certificate
  nginx-conf
  nginx-service
  nginx-frontend-promise
  llamacpp-frontend-promise
  request-parameters
  publish-connection-parameter

extends =
  {{ monitor_template_cfg }}

[versions]
fsspec = 2023.5.0
tqdm = 4.42.1
huggingface-hub = 0.30

[huggingface-cli]
recipe = zc.recipe.egg
eggs =
  fsspec
  tqdm
  huggingface_hub
scripts =
  huggingface-cli

[directory]
recipe = slapos.cookbook:mkdirectory
home = $${buildout:directory}
etc = $${:home}/etc
var = $${:home}/var
srv = $${:home}/srv
bin = $${:home}/bin
tmp = $${:home}/tmp
log = $${:var}/log
run = $${:var}/run
services = $${:etc}/service
scripts = $${:etc}/run
models = $${:var}/models
cache = $${:var}/hf-cache
httpd-log = $${:var}/log/apache
www = $${:srv}/www
ssl = $${:etc}/ssl

[huggingface-cli-download-script]
recipe = slapos.recipe.template:jinja2
url = {{ huggingface_cli_download_template }}
output = $${directory:bin}/huggingface-cli-download-run.py
mode = 0755
context =
    raw hfc_bin    $${buildout:bin-directory}/huggingface-cli
    raw models_dir $${directory:models}
    raw cache_dir  $${directory:cache}
    raw log_file   $${directory:log}/hf-download.log
    raw state_file $${directory:log}/huggingface-model-download.state
    raw repo_id    {{ slapparameter_dict.get('hf_repo_id', 'unsloth/Seed-OSS-36B-Instruct-GGUF') }}
    raw filename   {{ slapparameter_dict.get('hf_filename', 'Seed-OSS-36B-Instruct-IQ4_NL.gguf') }}

[huggingface-cli-download]
recipe = slapos.cookbook:wrapper
command-line = $${directory:bin}/huggingface-cli-download-run.py
wrapper-path = $${directory:services}/huggingface-cli-download
depends =
  $${huggingface-cli-download-script:recipe}

[huggingface-download-promise]
<= monitor-promise-base
promise = check_file_state
name = huggingface-download-complete.py
config-state = empty
filename = huggingface-model-download.state
config-filename = $${directory:log}/$${:filename}

[llamacpp-server-run-script]
recipe = slapos.recipe.template:jinja2
url = {{ llamacpp_server_run_template }}
output = $${directory:bin}/llamacpp-server-run.py
log_filename = $${directory:log}/llamacpp-server.log
mode = 0755
context =
    raw llama_server_bin ${llama-source:location}/build/bin/llama-server
    raw models_dir $${directory:models}
    raw log_file $${:log_filename}
    raw model {{ slapparameter_dict.get('hf_filename', 'gpt-oss-120b-F16.gguf') }}
    raw host $${llamacpp-service:llamacpp-host}
    raw port {{ slapparameter_dict.get('port', 8080) }}
    raw ngl {{ slapparameter_dict.get('ngl', 64) }}
    raw threads  {{ slapparameter_dict.get('threads', 16) }}
    raw threads_batch  {{ slapparameter_dict.get('threads_batch', 16) }}
    raw batch  {{ slapparameter_dict.get('batch', 1024) }}
    raw micro_batch  {{ slapparameter_dict.get('micro_batch', 1024) }}
    raw cache_type_k  {{ slapparameter_dict.get('cache_type_k', 'q8_0') }}
    raw cache_type_v  {{ slapparameter_dict.get('cache_type_v', 'q8_0') }}
    raw ctx {{ slapparameter_dict.get('ctx', 2048) }}
    raw repo_id {{ slapparameter_dict.get('hf_repo_id', 'unsloth/gpt-oss-120b-GGUF') }}

[llamacpp-service]
recipe = slapos.cookbook:wrapper
command-line = $${directory:bin}/llamacpp-server-run.py
wrapper-path = $${directory:services}/llamacpp-service
llamacpp-host = $${slap-configuration:ipv4-random}
wait-for-files =
  $${directory:models}/{{ slapparameter_dict.get('hf_filename', 'Seed-OSS-36B-Instruct-IQ4_NL.gguf') }}
environment =
  LD_LIBRARY_PATH=/usr/local/cuda/lib64:%(LD_LIBRARY_PATH)s
  HF_HOME=$${directory:cache}
  HUGGING_FACE_HUB_TOKEN={{ slapparameter_dict.get('hf_token', '') }}
depends =
  $${huggingface-cli-download:recipe}
  $${huggingface-download-promise:recipe}

[nginx-certificate]
recipe = plone.recipe.command
command =
  if [ ! -e $${:key-file} ]
  then
    mkdir -p $${directory:ssl}
    {{ openssl_location }}/bin/openssl req -x509 -nodes -sha256 -days 3650 \
      -subj "/C=AA/ST=X/L=X/O=Dis/CN=$${nginx-parameters:ipv6}" \
      -newkey rsa -keyout $${:key-file} \
      -out $${:cert-file}
  fi
update-command = $${:command}
key-file = $${nginx-parameters:ssl-key}
cert-file = $${nginx-parameters:ssl-certificate}
common-name = $${nginx-parameters:ipv6}
stop-on-error = true

[nginx-parameters]
ipv6 = $${slap-configuration:ipv6-random}
port = 4443
pid-file = $${directory:run}/nginx.pid
config-file = $${directory:etc}/nginx.conf
access-log = $${directory:httpd-log}/nginx-access.log
error-log = $${directory:httpd-log}/nginx-error.log
nginx-mime-types = {{ nginx_location }}/conf/mime.types
backend-url = http://$${llamacpp-service:llamacpp-host}:{{ slapparameter_dict.get('port', 8080) }}
ssl-certificate = $${directory:etc}/ssl/nginx.crt
ssl-key = $${directory:etc}/ssl/nginx.key

[nginx-conf]
recipe = slapos.recipe.template:jinja2
url = {{ nginx_conf_in }}
output = $${nginx-parameters:config-file}
context =
    section parameter_dict nginx-parameters

[nginx-service]
recipe = slapos.cookbook:wrapper
command-line = {{ nginx_location }}/sbin/nginx -c $${nginx-conf:output} -p $${directory:srv} -g "error_log $${directory:log}/nginx-startup.log debug;"
wrapper-path = $${directory:services}/nginx-service
wait-for-files =
  $${nginx-parameters:ssl-certificate}
  $${nginx-parameters:ssl-key}

[nginx-frontend-promise]
<= monitor-promise-base
promise = check_socket_listening
name = nginx-ipv6-port-listening.py
config-host = $${nginx-parameters:ipv6}
config-port = $${nginx-parameters:port}

[request-frontend]
<= monitor-frontend
name = lllamacpp Frontend
config-url = https://[$${nginx-parameters:ipv6}]:$${nginx-parameters:port}

[llamacpp-frontend-promise]
<= monitor-promise-base
promise = check_url_available
name = llamacpp-http-frontend.py
url = $${request-frontend:connection-secure_access}
config-url = $${:url}

[request-parameters]
recipe = slapos.cookbook:publish
url = http://{{ slapparameter_dict.get('host', '0.0.0.0') }}:{{ slapparameter_dict.get('port', 8080) }}/
model = {{ slapparameter_dict.get('hf_repo_id', 'unsloth/Seed-OSS-36B-Instruct-GGUF') }}:{{ slapparameter_dict.get('hf_filename', 'Seed-OSS-36B-Instruct-IQ4_NL.gguf') }}
ctx = {{ slapparameter_dict.get('ctx', 2048) }}
ngl = {{ slapparameter_dict.get('ngl', 64) }}

[publish-connection-parameter]
recipe = slapos.cookbook:publish
url = $${request-frontend:connection-secure_access}
ipv4_url =  http://$${llamacpp-service:llamacpp-host}:{{ slapparameter_dict.get('port', 8080) }}
backend-url = $${request-frontend:config-url}
repo = {{ slapparameter_dict.get('hf_repo_id', 'unsloth/Seed-OSS-36B-Instruct-GGUF') }}
filename = {{ slapparameter_dict.get('hf_filename', 'Seed-OSS-36B-Instruct-IQ4_NL.gguf') }}
model = $${:repo}:$${:filename}
model-download-log = $${directory:log}/hf-download.log
service-log = $${directory:log}/llamacpp-server.log
