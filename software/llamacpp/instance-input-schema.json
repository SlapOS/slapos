{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "description": "Parameters to instantiate Llama.cpp server",
  "additionalProperties": false,
  "required": [],
  "properties": {
    "port": {
      "title": "Port",
      "description": "Port on which to listen for incoming requests",
      "type": "integer",
      "default": 8083
    },
    "threads": {
      "title": "Threads",
      "description": "Number of threads to use for generation",
      "type": "integer",
      "default": 16
    },
    "ctx": {
      "title": "Context size",
      "description": "Context size for the model",
      "type": "integer",
      "default": 2048
    },
    "ngl": {
      "title": "Number of GPU layers",
      "description": "Number of layers to offload to GPU",
      "type": "integer",
      "default": 999
    },
    "hf_repo_id": {
      "title": "Hugging Face repository ID",
      "description": "Repository ID on Hugging Face Hub",
      "type": "string",
      "default": "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    },
    "hf_filename": {
      "title": "Hugging Face filename",
      "description": "Model filename on Hugging Face Hub",
      "type": "string",
      "default": "Qwen3-30B-A3B-Instruct-2507-UD-Q5_K_XL.gguf"
    },
    "hf_token": {
      "title": "Hugging Face token",
      "description": "Token for accessing private models on Hugging Face Hub",
      "type": "string",
      "default": ""
    }
  }
}
