{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "description": "Parameters to instantiate Llama.cpp server",
  "additionalProperties": false,
  "required": [],
  "properties": {
    "port": {
      "title": "Port",
      "description": "Port on which to listen for incoming requests",
      "type": "integer",
      "default": 8083
    },
    "threads": {
      "title": "Threads",
      "description": "Number of threads to use for generation",
      "type": "integer",
      "default": 16
    },
    "threads-batch": {
      "title": "Threads batch",
      "description": "Number of threads to use for prefill",
      "type": "integer",
      "default": 16
    },
    "ctx": {
      "title": "Context size",
      "description": "Context size for the model",
      "type": "integer",
      "default": 2048
    },
    "batch": {
      "title": "Batch size",
      "description": "Number of tokens processed together in a single forward pass during the prefill stage.",
      "type": "integer",
      "default": 1024
    },
    "micro-batch": {
      "title": "Micro-batch size",
      "description": "Subdivision of the main batch into smaller chunks to fit memory constraints.",
      "type": "integer",
      "default": 1024
    },
    "cache-type-k": {
      "title": "Key cache data type",
      "description": "Specifies the precision or quantization format used to store the Key vectors in the KV cache.",
      "type": "string",
      "default": "q8_0"
    },
    "cache-type-v": {
      "title": "Value cache data type",
      "description": "Specifies the precision or quantization format used to store the Value vectors in the KV cache.",
      "type": "string",
      "default": "q8_0"
    },
    "ngl": {
      "title": "Number of GPU layers",
      "description": "Number of layers to offload to GPU",
      "type": "integer",
      "default": 999
    },
    "dry-multiplier": {
      "title": "DRY sampling multiplier",
      "description": "set DRY sampling multiplier",
      "type": "number",
      "default": 0.0
    },
    "temp": {
      "title": "temperature",
      "description": "Temperature scales logits, sharpening or flattening probabilities to control randomness.",
      "type": "number",
      "default": 0.8
    },
    "top-k": {
      "title": "top-k sampling ",
      "description": "Top-k samples only from the K highest-probability tokens each step",
      "type": "integer",
      "default": 40
    },
    "top-p": {
      "title": "top-p sampling ",
      "description": "Top-p samples from the smallest set whose cumulative probability reaches p.",
      "type": "number",
      "default": 0.9
    },
    "min-p": {
      "title": "min-p sampling ",
      "description": "Min_p drops tokens below a fraction of the most likely token.",
      "type": "number",
      "default": 0.1
    },
    "reasoning-budget": {
      "title": "Reasoning budget",
      "description": "Limits model “thinking” tokens; 0 disables, -1 unlimited.",
      "type": "integer",
      "default": 0
    },
    "embedding": {
      "title": "Is embedding model",
      "description": "This service runs embedding model",
      "type": "integer",
      "default": 0
    },
    "hf_repo_id": {
      "title": "Hugging Face repository ID",
      "description": "Repository ID on Hugging Face Hub",
      "type": "string",
      "default": "unsloth/embeddinggemma-300m-GGUF"
    },
    "hf_filename": {
      "title": "Hugging Face filename",
      "description": "Model filename on Hugging Face Hub",
      "type": "string",
      "default": "embeddinggemma-300m-Q4_0.gguf"
    },
    "hf_token": {
      "title": "Hugging Face token",
      "description": "Token for accessing private models on Hugging Face Hub",
      "type": "string",
      "default": ""
    }
  }
}
