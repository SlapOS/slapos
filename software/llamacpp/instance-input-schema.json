{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "description": "Parameters to instantiate Llama.cpp server",
  "additionalProperties": false,
  "required": [],
  "properties": {
    "host": {
      "title": "Host address",
      "description": "Host address on which to listen for incoming requests",
      "type": "string",
      "default": "0.0.0.0"
    },
    "port": {
      "title": "Port",
      "description": "Port on which to listen for incoming requests",
      "type": "integer",
      "default": 8083
    },
    "threads": {
      "title": "Threads",
      "description": "Number of threads to use for generation",
      "type": "integer",
      "default": 16
    },
    "ctx": {
      "title": "Context size",
      "description": "Context size for the model",
      "type": "integer",
      "default": 2048
    },
    "ngl": {
      "title": "Number of GPU layers",
      "description": "Number of layers to offload to GPU",
      "type": "integer",
      "default": 64
    },
    "hf_repo_id": {
      "title": "Hugging Face repository ID",
      "description": "Repository ID on Hugging Face Hub",
      "type": "string",
      "default": "unsloth/gpt-oss-20b-GGUF"
    },
    "hf_filename": {
      "title": "Hugging Face filename",
      "description": "Model filename on Hugging Face Hub",
      "type": "string",
      "default": "F16"
    },
    "hf_token": {
      "title": "Hugging Face token",
      "description": "Token for accessing private models on Hugging Face Hub",
      "type": "string",
      "default": ""
    }
  }
}