[buildout]
parts =
  directory
  llamacpp
  request-parameters
  template-environment

[directory]
recipe = slapos.cookbook:mkdirectory
home = ${buildout:directory}
bin = ${:home}/bin
var = ${:home}/var
log = ${:var}/log
run = ${:var}/run
cache = ${:var}/hf-cache

[llamacpp]
recipe = slapos.cookbook:wrapper
command-line = ${template-environment:LLAMA_SERVER_BIN} \
  -hf {{ slapparameter_dict.get('hf_repo_id', 'unsloth/gpt-oss-20b-GGUF') }}:{{ slapparameter_dict.get('hf_filename', 'F16') }} \
  --host {{ slapparameter_dict.get('host', '0.0.0.0') }} --port {{ slapparameter_dict.get('port', 8083) }} \
  --ngl {{ slapparameter_dict.get('ngl', 64) }} -t {{ slapparameter_dict.get('threads', 16) }} -c {{ slapparameter_dict.get('ctx', 2048) }}
wrapper-path = ${directory:bin}/service-llama
environment =
  LD_LIBRARY_PATH=/usr/local/cuda/lib64:%(LD_LIBRARY_PATH)s
  HF_HOME=${directory:cache}
  HUGGING_FACE_HUB_TOKEN={{ slapparameter_dict.get('hf_token', '') }}

[request-parameters]
recipe = slapos.cookbook:publish
url = http://{{ slapparameter_dict.get('host', '0.0.0.0') }}:{{ slapparameter_dict.get('port', 8083) }}/
binary = ${template-environment:LLAMA_SERVER_BIN}
model = {{ slapparameter_dict.get('hf_repo_id', 'unsloth/gpt-oss-20b-GGUF') }}:{{ slapparameter_dict.get('hf_filename', 'F16') }}
ctx = {{ slapparameter_dict.get('ctx', 2048) }}
ngl = {{ slapparameter_dict.get('ngl', 64) }}

[template-environment]
recipe = plone.recipe.command
LLAMA_SERVER_BIN = {{ LLAMA_SERVER_BIN }}
stop-on-error = false
