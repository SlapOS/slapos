[buildout]
parts =
  promises
  publish-connection-parameter

eggs-directory = {{ buildout['eggs-directory'] }}
develop-eggs-directory = {{ buildout['develop-eggs-directory'] }}
offline = true


[instance-parameter]
recipe = slapos.cookbook:slapconfiguration
computer = ${slap-connection:computer-id}
partition = ${slap-connection:partition-id}
url = ${slap-connection:server-url}
key = ${slap-connection:key-file}
cert = ${slap-connection:cert-file}

[slap-configuration]
# apache-frontend reads from from a part named [slap-configuration]
recipe = slapos.cookbook:slapconfiguration.serialised
computer = ${slap-connection:computer-id}
partition = ${slap-connection:partition-id}
url = ${slap-connection:server-url}
key = ${slap-connection:key-file}
cert = ${slap-connection:cert-file}

[directory]
recipe = slapos.cookbook:mkdirectory
home = ${buildout:directory}
etc = ${:home}/etc
var = ${:home}/var
srv = ${:home}/srv
service = ${:etc}/service
promise = ${:etc}/promise
influxdb-data-dir = ${:srv}/influxdb
grafana-dir = ${:srv}/grafana
grafana-data-dir = ${:grafana-dir}/data
grafana-logs-dir = ${:var}/log
grafana-plugins-dir = ${:grafana-dir}/plugins
grafana-provisioning-config-dir = ${:grafana-dir}/provisioning-config
grafana-provisioning-datasources-dir = ${:grafana-provisioning-config-dir}/datasources
grafana-provisioning-dashboards-dir = ${:grafana-provisioning-config-dir}/dashboards
grafana-dashboards-dir = ${:grafana-dir}/dashboards
telegraf-dir = ${:srv}/telegraf
telegraf-extra-config-dir = ${:telegraf-dir}/extra-config
loki-dir = ${:srv}/loki
loki-storage-boltdb-dir = ${:loki-dir}/index/
loki-storage-filesystem-dir = ${:loki-dir}/chunks/
promtail-dir = ${:srv}/promtail

# macros
[generate-certificate]
recipe = plone.recipe.command
command =
  if [ ! -e ${:key-file} ]
  then
    {{ openssl_bin }} req -x509 -nodes -days 3650 \
      -subj "/C=AA/ST=X/L=X/O=Dis/CN=${:common-name}" \
      -newkey rsa:1024 -keyout ${:key-file} \
      -out ${:cert-file}
  fi
update-command = ${:command}
key-file = ${directory:etc}/${:_buildout_section_name_}.key
cert-file = ${directory:etc}/${:_buildout_section_name_}.crt
common-name = ${:_buildout_section_name_}

[config-file]
recipe = slapos.recipe.template:jinja2
template = {{ buildout['parts-directory'] }}/${:_buildout_section_name_}/${:_buildout_section_name_}.cfg.in
rendered = ${directory:etc}/${:_buildout_section_name_}.cfg
mode = 0644
extensions = jinja2.ext.do

[check-port-listening-promise]
recipe = slapos.cookbook:check_port_listening
path = ${directory:promise}/${:_buildout_section_name_}

[check-url-available-promise]
recipe = slapos.cookbook:check_url_available
path = ${directory:promise}/${:_buildout_section_name_}
dash_path = {{ dash_bin }}
curl_path = {{ curl_bin }}

[influxdb]
ipv6 = ${instance-parameter:ipv6-random}
ipv4 = ${instance-parameter:ipv4-random}
host = ${:ipv6}
local-host = ${:ipv4}
rpc-port = 8088
http-port = 8086
url = https://[${:host}]:${:http-port}
data-dir = ${directory:influxdb-data-dir}
auth-username = ${influxdb-password:username}
auth-password = ${influxdb-password:passwd}
unix-socket = ${directory:var}/influxdb.socket
ssl-cert-file = ${influxdb-certificate:cert-file}
ssl-key-file = ${influxdb-certificate:key-file}
database = telegraf

recipe = slapos.cookbook:wrapper
command-line =
   nice -19 chrt --idle 0 ionice -c3 {{ influxd_bin }} -config ${influxdb-config-file:rendered}
wrapper-path = ${directory:service}/influxdb

[influxdb-config-file]
<= config-file
context =
  section influxdb influxdb

[influxdb-password]
recipe = slapos.cookbook:generate.password
username = influxdb

[influxdb-certificate]
<= generate-certificate

[influxdb-listen-promise]
<= check-port-listening-promise
hostname = ${influxdb:ipv6}
port = ${influxdb:http-port}

[influxdb-password-promise]
recipe = slapos.cookbook:wrapper
command-line =
  {{ influx_bin }} -username ${influxdb:auth-username} -password ${influxdb:auth-password} -socket ${influxdb:unix-socket} -execute "CREATE USER ${influxdb:auth-username} WITH PASSWORD '${influxdb:auth-password}' WITH ALL PRIVILEGES"
wrapper-path = ${directory:promise}/${:_buildout_section_name_}

[influxdb-database-ready-promise]
recipe = slapos.cookbook:wrapper
command-line =
  bash -c "{{ influx_bin }} \
     -username ${influxdb:auth-username} \
     -password ${influxdb:auth-password} \
     -host [${influxdb:host}] \
     -port ${influxdb:http-port} \
     -unsafeSsl \
     -ssl \
     -execute 'show databases' | grep '${influxdb:database}'"
wrapper-path = ${directory:promise}/${:_buildout_section_name_}


[grafana]
ipv6 = ${instance-parameter:ipv6-random}
port = 8180
url = https://[${:ipv6}]:${:port}

data-dir = ${directory:grafana-data-dir}
logs-dir = ${directory:grafana-logs-dir}
plugins-dir = ${directory:grafana-plugins-dir}
provisioning-config-dir = ${directory:grafana-provisioning-config-dir}
provisioning-datasources-dir = ${directory:grafana-provisioning-datasources-dir}
provisioning-dashboards-dir = ${directory:grafana-provisioning-dashboards-dir}
admin-user = ${grafana-password:username}
admin-password = ${grafana-password:passwd}
secret-key = ${grafana-secret-key:passwd}
ssl-key-file = ${grafana-certificate:key-file}
ssl-cert-file = ${grafana-certificate:cert-file}

recipe = slapos.cookbook:wrapper
command-line =
    {{ grafana_bin }} -config ${grafana-config-file:rendered} -homepath {{ grafana_homepath }}
wrapper-path = ${directory:service}/grafana

[grafana-certificate]
<= generate-certificate

[grafana-password]
recipe = slapos.cookbook:generate.password
username = admin

[grafana-secret-key]
recipe = slapos.cookbook:generate.password

[grafana-config-file]
<= config-file
context =
  section grafana grafana
  section apache_frontend apache-frontend
  key slapparameter_dict slap-configuration:configuration
depends =
  ${grafana-provisioning-datasources-config-file:rendered}
  ${grafana-provisioning-dashboards-config-file:rendered}

[grafana-provisioning-datasources-config-file]
<= config-file
rendered = ${grafana:provisioning-datasources-dir}/datasource.yaml
context =
  section influxdb influxdb
  section loki loki

[grafana-provisioning-dashboards-config-file]
<= config-file
rendered = ${grafana:provisioning-dashboards-dir}/dashboard.yaml
context =
  key dashboards_dir directory:grafana-dashboards-dir

[grafana-listen-promise]
<= check-port-listening-promise
hostname= ${grafana:ipv6}
port = ${grafana:port}

[telegraf]
recipe = slapos.cookbook:wrapper
extra-config-dir = ${directory:telegraf-extra-config-dir}
# telegraf needs influxdb to be already listening before starting
command-line =
   bash -c '${influxdb-listen-promise:path} && nice -19 chrt --idle 0 ionice -c3 {{ telegraf_bin }} --config ${telegraf-config-file:rendered} --config-directory ${:extra-config-dir}'
wrapper-path = ${directory:service}/telegraf

[telegraf-config-file]
<= config-file
context =
  section influxdb influxdb
  section telegraf telegraf
  section extra telegraf-config-file-extra

[telegraf-config-file-extra]
recipe = slapos.recipe.build
telegraf-input-slapos-bin = {{ telegraf_input_slapos_bin }}
slapparameter-dict = ${slap-configuration:configuration}
init =
  import zc.buildout
  import pkg_resources

  buildout_options = self.buildout["buildout"]
  zc.buildout.easy_install.install(
    ["toml"],
    dest=None,
    working_set=pkg_resources.working_set,
    path=[
      buildout_options["develop-eggs-directory"],
      buildout_options["eggs-directory"],
    ],
  )

  import collections
  import os.path
  import urllib.parse
  import toml

  # files to create during install step
  self._config_files = {}

  inputs = collections.defaultdict(list)
  processors = collections.defaultdict(list)
  slapparameter_dict = self.options["slapparameter-dict"]
  for application in slapparameter_dict.get('applications', []):
    partition_mapping = {}
    for partition in application.get("partitions", []):
      partition.setdefault("type", "default")
      if "reference" in partition:
        partition_mapping[partition["reference"]] = partition["name"]
        partition_directory = os.path.join(application["instance-root"], partition['reference'])
      if partition["type"] in ("erp5/mariadb", "mariadb"):
        partition.setdefault("username", "root")
        partition.setdefault("dbname", "erp5")
        dsn = f"{partition['username']}@unix({partition_directory}/var/run/mariadb.sock)/{partition['dbname']}"
        inputs["mysql"].append(
          {
            "name_override": f"{partition['name']}-mysql",
            "servers": [dsn],
            "gather_innodb_metrics": True,
            "tags": dict(partition.get("static-tags", {}), app=application["name"]),
          }
        )
        if partition["type"] == "erp5/mariadb":
          inputs["sql"].append(
            {
              "name_override": f"{partition['name']}-activities",
              "driver": "mysql",
              "dsn": dsn,
              "query": [
                {
                  "query": "select count(*) as message_count from message",
                  "field_columns_include": ["message_count"],
                },
                {
                  "query": "select count(*) as message_queue_count from message_queue",
                  "field_columns_include": ["message_queue_count"],
                },
                {
                  "query": "select count(*) as message_failed_count from message where processing_node=-2",
                  "field_columns_include": ["message_failed_count"],
                },
                {
                  "query": "select count(*) as message_queue_failed_count from message_queue where processing_node=-2",
                  "field_columns_include": ["message_queue_failed_count"],
                },
                {
                  "query": """
                    select cast(coalesce(max(UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(message.date)), 0) as int)
                      as message_waiting_time from message
                      where processing_node in (-1, 0) and message not like '%after_tag%'
                  """,
                  "field_columns_include": ["message_waiting_time"],
                },
                {
                  "query": """
                    select cast(coalesce(max(UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(message_queue.date)), 0) as int)
                      as message_queue_waiting_time from message_queue
                      where processing_node in (-1, 0) and message not like '%after_tag%'
                  """,
                  "field_columns_include": ["message_queue_waiting_time"],
                }
              ],
              "tags": dict(partition.get("static-tags", {}), app=application["name"]),
            }
          )

      if partition["type"] == "erp5/balancer":
        inputs["tail"].append(
          {
            "data_format": "grok",
            "files": [f"{partition_directory}/var/log/apache-access.log"],
            "grok_custom_pattern_files": [],
            "grok_custom_patterns": "",
            "grok_patterns": [
              '%{IPORHOST:client_ip} %{NOTSPACE:ident} %{NOTSPACE:auth} \\[%{HTTPDATE:timestamp}\\] "(?:%{WORD:verb:tag} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version:float})?|%{DATA})" %{NUMBER:resp_code:tag} (?:%{NUMBER:resp_bytes:int}|-) %{QS:referrer} %{QS:agent} %{NUMBER:response_time:int}'
            ],
            "grok_timezone": "Local",
            "name_override": f"{partition['name']}",
            "tags": dict(partition.get("static-tags", {}), app=application["name"]),
          }
        )
    urls = application.get("urls", [])
    if urls:
      inputs["http_response"].append({
        "interval": "5m",
        "urls": urls,
        "tags": {"app": application["name"]},
      })
      
    for url in urls:
      x509_url = url
      parsed_url = urllib.parse.urlparse(url)
      if parsed_url.scheme == 'https':
        # x509_cert wants a port
        if not parsed_url.port:
          x509_url = parsed_url._replace(netloc=parsed_url.hostname+':443').geturl()
      inputs["x509_cert"].append({
        "sources": [x509_url],
        "tags": {"url": url},
        "interval": "5h",
        "tags": {"app": application["name"]},
      })


    # TODO: don't run more than one per instance_root
    telegraf_slapos_input_config_file = os.path.join(
        self.options['location'],
        f"telegraf-input-slapos-{application['name']}.cfg")
    self._config_files[telegraf_slapos_input_config_file] = toml.dumps({
      "inputs": {
        "slapos": [{
          "instance_root": application['instance-root']}]}})

    # TODO: supervisor process finder for
    # https://github.com/influxdata/telegraf/tree/master/plugins/inputs/procstat ? 
    telegraf_slapos_input_command = self.options['telegraf-input-slapos-bin']
    inputs["execd"].append({
      "name_override": f"{application['name']}-processes",
      "command": [telegraf_slapos_input_command, '-config', telegraf_slapos_input_config_file],
      "tags": {"app": application["name"]},
    })
    # "cleanup" slapos process names, remove hash from wrappers and -on-watch suffix
    processors["regex"].append({
      "namepass": [f"{application['name']}-processes"],
      "order": 1,
      "tags": [{
        "key": "name",
        "pattern": "^(.*)-.{32}",
         # XXX we concatenate strings so that we don't have to escape them for buildout
        "replacement": "$" + "{1}",
      }]})
    processors["regex"].append({
      "namepass": [f"{application['name']}-processes"],
      "order": 2,
      "tags": [{
        "key": "name",
        "pattern": "^(.*)-on-watch$",
        "replacement": "$" + "{1}",
      }]})
    processors["enum"].append({
      "namepass": [ f"{application['name']}-processes"],
      "mapping": [{
#        "tag": "group", # TODO: rename this in input plugin
        "tag": "slappart",
        "dest": "partition",
        "value_mappings": partition_mapping,
      }]})

  # TODOs:
  #  - [ ] slapos input
  #    - [x] friendly name of slappart
  #    - [x] strip hashes from -on-watch
  #  - [x] activity metrics
  #  - [ ] alert dashboard
  #  - [ ] inclu "jerome-dev" partout ???
  #  - [ ] apdex

  options["extra-config"] = toml.dumps({
    "inputs": inputs,
    "processors": processors})
  # import pdb; pdb.set_trace()

# apdex
# SELECT sum("success") / sum("all") FROM 
#     (SELECT count("duration") AS "all" FROM "jerome-dev-balancer" WHERE $timeFilter GROUP BY time($__interval) fill(null)),
#     (SELECT count("duration")  AS "success" FROM "jerome-dev-balancer" WHERE  ("resp_code"  = '200' ) AND $timeFilter GROUP BY time($__interval) fill(null))

#SELECT sum("success") + sum("all") FROM 
#     (SELECT count("duration") AS "all" FROM "jerome-dev-balancer" WHERE $timeFilter GROUP BY time($__interval) fill(0)),
#     (SELECT count("duration")  AS "success" FROM "jerome-dev-balancer" WHERE  ("resp_code"  = '200' ) AND $timeFilter GROUP BY time($__interval) fill(0))


install =
  import os
  os.mkdir(self.options['location'])
  for fname, content in self._config_files.items():
    with open(fname, 'w') as f:
      f.write(content)


[loki]
recipe = slapos.cookbook:wrapper
command-line =
   bash -c 'nice -19 chrt --idle 0 ionice -c3 {{ loki_bin }} -config.file=${loki-config-file:rendered}'
wrapper-path = ${directory:service}/loki

storage-boltdb-dir = ${directory:loki-storage-boltdb-dir}
storage-filesystem-dir = ${directory:loki-storage-filesystem-dir}
ip = ${instance-parameter:ipv4-random}
port = 3100
grpc-port = 9095
url = http://${:ip}:${:port}


[loki-config-file]
<= config-file
context =
  section loki loki

[loki-listen-promise]
<= check-url-available-promise
url = ${loki:url}/ready

[promtail]
recipe = slapos.cookbook:wrapper
command-line =
   bash -c 'nice -19 chrt --idle 0 ionice -c3 {{ promtail_bin }} -config.file=${promtail-config-file:location}'
wrapper-path = ${directory:service}/promtail

dir = ${directory:promtail-dir}
http-port = 19080
grpc-port = 19095
ip = ${instance-parameter:ipv4-random}
url = http://${:ip}:${:http-port}

[promtail-config-file]
recipe = slapos.recipe.build
location = ${directory:etc}/${:_buildout_section_name_}.cfg
slapparameter-dict = ${slap-configuration:configuration}
install =
{% raw %}
  import os
  # XXX make extra eggs available to buildout
  import zc.buildout
  import pkg_resources
  buildout_options = self.buildout['buildout']
  zc.buildout.easy_install.install(
      ['pyyaml'],
      dest=None,
      working_set=pkg_resources.working_set,
      path=[
          buildout_options['develop-eggs-directory'],
          buildout_options['eggs-directory']])

  import yaml
  slapparameter_dict = self.options['slapparameter-dict']
  cfg = {
      "server": {
          "http_listen_address": self.buildout['promtail']['ip'],
          "http_listen_port": int(self.buildout['promtail']['http-port']),
          "grpc_listen_address": self.buildout['promtail']['ip'],
          "grpc_listen_port": int(self.buildout['promtail']['grpc-port']),
          "external_url": self.buildout['promtail']['url'],
      },
      "positions": {
          "filename": "{}/positions.yaml".format(self.buildout['promtail']['dir']),
      },
      "clients": [
          {
              "url": "{}/api/prom/push".format(self.buildout['loki']['url']),
          }
      ],
      "scrape_configs": []
  }
  def get_job_selector(partition, job_name, application_name):
    # make a selector in LogQL, like '{job="job_name",key="value"}'
    selector_parts = [f'app="{application_name}"']
    for k, v in dict(partition.get('static-tags', {}), job=job_name).items():
      selector_parts.append(f'{k}="{v}"')
    return "{%s}" % ",".join(selector_parts)

  def get_static_configs(partition, job_name, path, application):
    directory = ''
    if partition.get('reference'):
      directory = os.path.join(application['instance-root'], partition['reference'])
    return [
        {
            "targets": [
                "localhost"
            ],
            "labels": dict(
                partition.get('static-tags', {}),
                job=job_name,
                app=application['name'],
                __path__=path.format(directory=directory),
            )
        }
      ]

  for application in slapparameter_dict.get('applications', []):
    for partition in application.get('partitions', []):
      partition.setdefault("type", "default")
      if partition['type'] in ('erp5/zope-activity', 'erp5/zope-front'):
        job_name = f"{partition['name']}-event-log"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "pipeline_stages": [
              {
                "match": {
                  "selector": get_job_selector(partition, job_name, application['name']),
                  "stages": [
                    {
                      "multiline": {
                        "firstline": "^------",
                        "max_wait_time": "3s"
                      }
                    },
                    {
                      "regex": {
                        "expression": "^------\\n(?P<timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{1,2}\\:\\d{2}\\:\\d{2}\\,\\d{3}) (?P<level>\\S+) (?P<component>\\S+) (?P<message>.*)"
                      }
                    },
                    {
                      "timestamp": {
                        "format": "2021-04-04 03:57:11,242",
                        "source": "timestamp"
                      }
                    },
                    {
                      "labels": {
                        "level": None,
                        "component": None
                      }
                    }
                  ]
                }
              }
            ],
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/zope-*-event.log",
                application,
            )})
        if partition['type'] == 'erp5/zope-front':
          job_name = f"{partition['name']}-access-log"
          cfg['scrape_configs'].append({
              "job_name": job_name,
              # drop requests for haproxy health check
              "pipeline_stages": [
                  {
                      "drop": {
                          "expression": '.* "GET / HTTP/1.0" 200 .*' 
                      }
                  }
              ],
              "static_configs": get_static_configs(
                  partition,
                  job_name,
                  "{directory}/var/log/zope-*-Z2.log",
                  application,
              )})
          job_name = f"{partition['name']}-long-request-log"
          cfg['scrape_configs'].append({
              "job_name": job_name,
              "pipeline_stages": [
                {
                  "match": {
                    "selector": get_job_selector(partition, job_name, application['name']),
                    "stages": [
                      {
                        "multiline": {
                          "firstline": "^\\d{4}-\\d{2}-\\d{2}\\s\\d{1,2}\\:\\d{2}\\:\\d{2}\\,\\d{3}",
                          "max_wait_time": "3s"
                        }
                      },
                      {
                        "regex": {
                          "expression": "^(?P<timestamp>.*) .*"
                        }
                      },
                      {
                        "timestamp": {
                          "format": "2021-04-04 03:57:11,242",
                          "source": "timestamp"
                        }
                      }
                    ]
                  }
                }
              ],
              "static_configs": get_static_configs(
                  partition,
                  job_name,
                  "{directory}/var/log/zope-*-longrequest.log",
                  application,
              )})
      if partition['type'] in ('erp5/mariadb', 'mariadb'):
        job_name = f"{partition['name']}-mariadb-slow-queries"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "pipeline_stages": [
              {
                "match": {
                  "selector": get_job_selector(partition, job_name, application['name']),
                  "stages": [
                    {
                      "multiline": {
                        # TODO
                        #"firstline": "^# Time: \\d{2}\\d{2}\\d{2}\\s\\d{1,2}\\:\\d{2}\\:\\d{2}",
                        "firstline": r"^# Time: \d{2}.*",
                        "max_wait_time": "3s"
                      }
                    },
                    {
                      "regex": {
                        "expression": ".*SET timestamp=(?P<timestamp>\\d+);.*"
                      }
                    },
                    {
                      "timestamp": {
                        "format": "Unix",
                        "source": "timestamp"
                      }
                    }
                  ]
                }
              }
            ],
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/mariadb_slowquery.log",
                application,
            )})
        job_name = f"{partition['name']}-mariadb-error-log"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "pipeline_stages": [
              {
                "match": {
                  "selector": get_job_selector(partition, job_name, application['name']),
                  "stages": [
                      {
                        "timestamp": {
                          "format": "2021-06-05  3:55:31",
                          "source": "timestamp"
                        }
                      }
                  ]
                }
              }
            ],
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/mariadb_error.log",
                application,
            )})
      if partition['type'] == 'erp5/zeo':
        job_name = f"{partition['name']}-zeo-log"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "pipeline_stages": [
              {
                "match": {
                  "selector": get_job_selector(partition, job_name, application['name']),
                  "stages": [
                      {
                        "multiline": {
                          "firstline": "^------",
                          "max_wait_time": "3s"
                        }
                      },
                      {
                        "regex": {
                          "expression": "^------\\n(?P<timestamp>\\d{4}-\\d{2}-\\d{2}\\s\\d{1,2}\\:\\d{2}\\:\\d{2}\\,\\d{3}) (?P<level>\\S+) (?P<component>\\S+) (?P<message>.*)"
                        }
                      },
                      {
                        "timestamp": {
                          "format": "2021-04-04 03:57:11,242",
                          "source": "timestamp"
                        }
                      },
                      {
                        "labels": {
                          "level": None,
                          "component": None
                        }
                      }
                    ]
                  }
                }
            ],
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/zeo-*.log",
                application,
            )})
      if partition['type'] == 'erp5/balancer':
        job_name = f"{partition['name']}-balancer-access-log"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/apache-access.log",
                application,
            )})
        job_name = f"{partition['name']}-balancer-error-log"
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "static_configs": get_static_configs(
                partition,
                job_name,
                "{directory}/var/log/apache-error.log",
                application,
            )})

      if partition.get('file-path'):
        job_name = partition['name']
        cfg['scrape_configs'].append({
            "job_name": job_name,
            "static_configs": get_static_configs(
                partition,
                job_name,
                f"{partition['file-path']}",
                application,
            )})

  with open(self.options['location'], 'w') as f:
    yaml.dump(cfg, f)

{% endraw %}

[promtail-listen-promise]
<= check-port-listening-promise
hostname= ${promtail:ip}
port = ${promtail:http-port}


[apache-frontend]
<= slap-connection
recipe = slapos.cookbook:requestoptional
name = Grafana Frontend
# XXX We have hardcoded SR URL here.
software-url = http://git.erp5.org/gitweb/slapos.git/blob_plain/HEAD:/software/apache-frontend/software.cfg
slave = true
config-url = ${grafana:url}
config-https-only = true
return = domain secure_access

[apache-frontend-available-promise]
<= check-url-available-promise
url = ${apache-frontend:connection-secure_access}


[promises]
recipe =
instance-promises =
  ${influxdb-listen-promise:path}
  ${influxdb-password-promise:wrapper-path}
  ${influxdb-database-ready-promise:wrapper-path}
  ${grafana-listen-promise:path}
  ${loki-listen-promise:path}
  ${promtail-listen-promise:path}
  ${promtail-listen-promise:path}
  ${apache-frontend-available-promise:path}


[publish-connection-parameter]
recipe = slapos.cookbook:publish
influxdb-url = ${influxdb:url}
influxdb-database = ${influxdb:database}
influxdb-username = ${influxdb:auth-username}
influxdb-password = ${influxdb:auth-password}
telegraf-extra-config-dir = ${telegraf:extra-config-dir}
grafana-url = ${grafana:url}
grafana-username = ${grafana:admin-user}
grafana-password = ${grafana:admin-password}
loki-url = ${loki:url}
promtail-url = ${promtail:url}
url = ${apache-frontend:connection-secure_access}
