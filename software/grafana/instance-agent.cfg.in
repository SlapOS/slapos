{% import "caucase" as caucase with context %}

[buildout]
parts =
  promises
  publish-connection-parameter

eggs-directory = {{ buildout_eggs_directory }}
develop-eggs-directory = {{ buildout_develop_eggs_directory }}
offline = true


[instance-parameter]
recipe = slapos.cookbook:slapconfiguration
computer = ${slap-connection:computer-id}
partition = ${slap-connection:partition-id}
url = ${slap-connection:server-url}
key = ${slap-connection:key-file}
cert = ${slap-connection:cert-file}

[slap-configuration]
# apache-frontend reads from from a part named [slap-configuration]
recipe = slapos.cookbook:slapconfiguration.serialised
computer = ${slap-connection:computer-id}
partition = ${slap-connection:partition-id}
url = ${slap-connection:server-url}
key = ${slap-connection:key-file}
cert = ${slap-connection:cert-file}

[directory]
recipe = slapos.cookbook:mkdirectory
home = ${buildout:directory}
bin = ${:home}/bin
etc = ${:home}/etc
var = ${:home}/var
tmp = ${:home}/tmp
srv = ${:home}/srv
service = ${:etc}/service
promise = ${:etc}/promise
plugins = ${:etc}/plugin
telegraf-dir = ${:srv}/telegraf
telegraf-extra-config-dir = ${:telegraf-dir}/extra-config
caucase-updater-loki-fluent-bit-client = ${:srv}/caucase-updater/loki-client-fluent-bit
fluent-bit-dir = ${:srv}/fluent-bit


# macros
[config-file]
recipe = slapos.recipe.template:jinja2
url = {{ buildout_parts_directory }}/${:_buildout_section_name_}/${:_buildout_section_name_}.cfg.in
output = ${directory:etc}/${:_buildout_section_name_}.cfg
extensions = jinja2.ext.do

[check-port-listening-promise]
recipe = slapos.cookbook:check_port_listening
path = ${directory:promise}/${:_buildout_section_name_}

[check-url-available-promise]
recipe = slapos.cookbook:check_url_available
path = ${directory:promise}/${:_buildout_section_name_}
dash_path = {{ dash_bin }}
curl_path = {{ curl_bin }}

[influxdb-server]
recipe = slapos.recipe.build
slapparameter-dict = ${slap-configuration:configuration}
init =
  import urllib.parse
  influxdb = options['slapparameter-dict']['influxdb']
  options['url'] = influxdb['url']
  options['database'] = influxdb['database']
  options['auth-username'] = influxdb['username']
  options['auth-password'] = influxdb['password']
  parsed_url = urllib.parse.urlparse(options['url'])
  options['hostname'] = parsed_url.hostname
  options['port'] = str(parsed_url.port)

[influxdb-listen-promise]
<= check-port-listening-promise
hostname = ${influxdb-server:hostname}
port = ${influxdb-server:port}

[telegraf]
recipe = slapos.cookbook:wrapper
extra-config-dir = ${directory:telegraf-extra-config-dir}
# telegraf needs influxdb to be already listening before starting, so we wrap this command in bash.
# we also run a login shell so that $PATH is initialized and sensors plugin can find sensors command.
command-line =
   bash --login -c '${influxdb-listen-promise:path} && ${:nice} {{ telegraf_bin }} --config ${telegraf-config-file:output} --config-directory ${:extra-config-dir}'
wrapper-path = ${directory:service}/telegraf
hash-files = ${telegraf-config-file:output}
# TODO: control nice of the agent ?
{% if 0 %}
nice = nice -19 chrt --idle 0 ionice -c3
{% else %}
nice =
{% endif %}


[telegraf-config-file]
recipe = slapos.recipe.build
output = ${directory:etc}/${:_buildout_section_name_}.toml
telegraf-input-slapos-bin = {{ telegraf_input_slapos_bin }}
slapparameter-dict = ${slap-configuration:configuration}
input-socket = ${directory:var}/tg.sock
init =
  import zc.buildout
  import pkg_resources

  buildout_options = self.buildout["buildout"]
  zc.buildout.easy_install.install(
    ["toml"],
    dest=None,
    working_set=pkg_resources.working_set,
    path=[
      buildout_options["develop-eggs-directory"],
      buildout_options["eggs-directory"]])

  import collections
  import pathlib
  import urllib.parse
  import toml

  slapparameter_dict = self.options["slapparameter-dict"]
  slap_connection = self.buildout["slap-connection"]
  influxdb = self.buildout['influxdb-server']

  self._config_files = {}  # files to create during install step
  access_path_dict = {}
  inputs = collections.defaultdict(list)
  processors = collections.defaultdict(list)
  config = {
    "agent": {
      "debug": False,
      "flush_interval": "10s",
      "flush_jitter": "0s",
      "hostname": "",
      "interval": "10s",
      "round_interval": True,
    },
    "tags": {
      "computer_id": slap_connection['computer-id'],
    },
    "inputs": inputs,
    "processors": processors,

    "outputs": {
      "influxdb": {
        "database": influxdb["database"],
        "insecure_skip_verify": True,  # TODO
        "username": influxdb["auth-username"],
        "password": influxdb["auth-password"],
        "precision": "s",
        "urls": [
          influxdb["url"],
        ],
      },
    },
  }
  # built-in inputs
  inputs["cpu"].append(
    {
      "drop": ["cpu_time"],
      "percpu": True,
      "totalcpu": True,
    }
  )
  inputs["disk"].append({})
  inputs["diskio"].append({})
  inputs["mdstat"].append({"interval": "1h"})
  inputs["mem"].append({})
  inputs["net"].append({"ignore_protocol_stats": True})
  inputs["sensors"].append({"interval": "5m"})
  inputs["system"].append({})

  for application in slapparameter_dict.get("applications", []):
    partition_mapping = {}
    partition_root_directory = ''
    for partition in application.get("partitions", []):
      partition.setdefault("type", "default")
      if "reference" in partition:
        partition_mapping[partition["reference"]] = partition["name"]
        if application.get("instance-root"):
          partition_root_directory = pathlib.Path(application["instance-root"]) / partition['reference']
      if partition["type"] in ("erp5/mariadb", "mariadb"):
        partition.setdefault("username", "root")
        partition.setdefault("dbname", "erp5")
        mariadb_socket = f"{partition_root_directory}/var/run/mariadb.sock"
        dsn = f"{partition['username']}@unix({mariadb_socket})/{partition['dbname']}"
        access_path_dict[mariadb_socket] = 'rw'
        inputs["mysql"].append(
          {
            "name_override": "mariadb",
            "servers": [dsn],
            "gather_innodb_metrics": True,
            "gather_slave_status": True,
            "mariadb_dialect": True,
            "tags": dict(
              partition.get("static-tags", {}),
              app=application["name"],
              name=partition["name"],
              partition_reference=partition["reference"],
            ),
          }
        )
        if partition["type"] == "erp5/mariadb":
          inputs["sql"].append(
            {
              "name_override": "mariadb_activities",
              "driver": "mysql",
              "dsn": dsn,
              "query": [
                {
                  "query": """
                    select 'message' as cmf_activity_queue, count(*) as message_count from message
                    union all select 'message_queue' as cmf_activity_queue, count(*) as message_count from message_queue
                  """,
                  "field_columns_include": ["message_count"],
                  "tag_columns_include": ["cmf_activity_queue"],
                },
                {
                  "query": """
                    select 'message' as cmf_activity_queue, method_id, count(*) as failed_message_count
                      from message where processing_node between -10 and -2 group by method_id
                    union all select 'message_queue' as cmf_activity_queue, method_id, count(*) as failed_message_count
                      from message_queue where processing_node between -10 and -2 group by method_id
                  """,
                  "field_columns_include": ["failed_message_count"],
                  "tag_columns_include": ["cmf_activity_queue", "method_id"],
                },
                # TODO: these queries are slow and maybe not correct
                # {
                #   "query": """
                #     select cast(coalesce(max(UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(message.date)), 0) as int)
                #       as waiting_time, 'message' as cmf_activity_queue
                #       from message where processing_node in (-1, 0) and message.message not like '%after_tag%'
                #     union all
                #     select cast(coalesce(max(UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(message_queue.date)), 0) as int)
                #       as waiting_time, 'message_queue' as cmf_activity_queue
                #       from message_queue where processing_node in (-1, 0) and message_queue.message not like '%after_tag%'
                #   """,
                #   "field_columns_include": ["waiting_time"],
                #   "tag_columns_include": ["cmf_activity_queue"],
                # },
              ],
              "tags": dict(
                partition.get("static-tags", {}),
                app=application["name"],
                name=partition["name"],
                partition_reference=partition["reference"],
              ),
            }
          )

      if partition["type"] == "erp5/balancer":
        # XXX this produces many measurements
        haproxy_socket = f"{partition_root_directory}/var/run/ha.sock"
        access_path_dict[haproxy_socket] = 'rw'
        inputs["haproxy"].append(
          {
            "servers": [haproxy_socket],
            "tags": dict(
              partition.get("static-tags", {}),
              app=application["name"],
              name=partition["name"],
              partition_reference=partition["reference"],
            ),
          })
    urls = application.get("urls", [])
    if urls:
      inputs["http_response"].append({
        "interval": "2m",
        "response_timeout": "1m",
        "urls": urls,
        "tags": {"app": application["name"]},
      })

    for url in urls:
      x509_url = url
      parsed_url = urllib.parse.urlparse(url)
      if parsed_url.scheme == 'https':
        # x509_cert wants a port
        if not parsed_url.port:
          x509_url = parsed_url._replace(netloc=parsed_url.hostname+':443').geturl()
        inputs["x509_cert"].append({
          "sources": [x509_url],
          "tags": {"url": url},
          "interval": "5h",
          "tags": {"app": application["name"]},
        })

    if application.get("type") == "SlapOS":
      telegraf_slapos_input_config_file = str(
        pathlib.Path(self.options['location'])
        / f"telegraf-input-slapos-{application['name']}.cfg"
      )
      self._config_files[telegraf_slapos_input_config_file] = toml.dumps({
        "inputs": {
          "slapos": [{
            "instance_root": application["instance-root"]}]}})
      access_path_dict[f"{application['instance-root']}/sv.sock"] = 'rw'
      telegraf_slapos_input_command = self.options['telegraf-input-slapos-bin']
      inputs["execd"].append({
        "name_override": "slapos_services",
        "command": [telegraf_slapos_input_command, '-config', telegraf_slapos_input_config_file],
        "tags": {"app": application["name"]},
      })
      # drop measurements for not monitored partitions.
      processors["starlark"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 1,
        "source": f'''
  def apply(metric):
    if metric.tags.get('reference') in {list(partition_mapping)!r}:
      return metric
  '''
        })
      # telegraf-input-slapos outputs the process name as "name", but we rename
      # this to "process_name", so that it is more understandable in a global
      # context and because we use the name of the partition as "name" everywhere
      # else.
      processors["rename"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 2,
        "replace": [{
          "tag": "name",
          "dest": "process_name",
        }]})
      # "normalize" slapos process names, remove hash from hash-files and -on-watch suffix
      processors["regex"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 3,
        "tags": [{
          "key": "process_name",
          "pattern": "^(.*)-on-watch$",
          "replacement": "$" + "{1}",
        }]})
      processors["regex"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 4,
        "tags": [{
          "key": "process_name",
          "pattern": "^(.*)-\\w{32}",
           # XXX we concatenate strings so that we don't have to escape them for buildout
          "replacement": "$" + "{1}",
        }]})
      # use consistent `partition_reference` for slappart
      processors["rename"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 5,
        "replace": [{
          "tag": "reference",
          "dest": "partition_reference",
        }]})
      processors["enum"].append({
        "namepass": ["slapos_services"],
        "tagpass": {"app": [application["name"]]},
        "order": 6,
        "mapping": [{
          "tag": "partition_reference",
          "dest": "name",
          "value_mappings": partition_mapping,
        }]})

  # add a socket input so that we can have a promise verifying that telegraf is listening
  inputs['socket_listener'].append({"service_address": f"unix://{self.options['input-socket']}"})

  options['access-path-dict'] = access_path_dict
  self._config_files[options['output']] = toml.dumps(config)

install =
  import os
  os.mkdir(self.options['location'])
  for fname, content in self._config_files.items():
    with open(fname, 'w') as f:
      f.write(content)

[loki-server]
recipe = slapos.recipe.build
slapparameter-dict = ${slap-configuration:configuration}
init =
  import urllib.parse
  loki = options['slapparameter-dict']['loki']
  options['url'] = loki['url']
  options['caucase-url'] = loki['caucase-url']
  parsed_url = urllib.parse.urlparse(loki['url'])
  options['hostname'] = parsed_url.hostname
  options['port'] = parsed_url.port

[loki-client-certificate]
key-file = ${directory:etc}/${:_buildout_section_name_}.key
cert-file = ${directory:etc}/${:_buildout_section_name_}.crt
common-name = ${:_buildout_section_name_}
ca-file = ${directory:etc}/${:_buildout_section_name_}.ca.crt
crl-file = ${directory:etc}/${:_buildout_section_name_}.crl

[loki-client-certificate-csr-config]
recipe = slapos.recipe.template
inline =
  [req]
  prompt = no
  distinguished_name = dn
  [ dn ]
  CN = ${:cn}
  L = ${slap-connection:computer-id}
  O = ${slap-connection:partition-id}
output = ${buildout:parts-directory}/${:_buildout_section_name_}/${:_buildout_section_name_}

[loki-client-certificate-prepare-csr]
# variable
config =
recipe = plone.recipe.command
command =
  if [ ! -f '${:csr}' ] ; then
   {{ openssl_bin }} req \
      -newkey rsa \
      -batch \
      -new \
      -sha256 \
      -nodes \
      -keyout /dev/null \
      -config '${:config}' \
      -out '${:csr}'
  fi
stop-on-error = true
csr = ${directory:srv}/${:_buildout_section_name_}.csr.pem


[loki-fluent-bit-client-certificate]
<= loki-client-certificate
[loki-fluent-bit-client-certificate-csr-config]
<= loki-client-certificate-csr-config
cn = loki ${slap-connection:partition-id}@${slap-connection:computer-id}
[loki-fluent-bit-client-certificate-prepare-csr]
<= loki-client-certificate-prepare-csr
config = ${loki-fluent-bit-client-certificate-csr-config:output}
{{
caucase.updater(
    prefix='loki-fluent-bit-client-certificate',
    buildout_bin_directory=buildout_bin_directory,
    updater_path='${directory:service}/loki-fluent-bit-client-certificate-updater',
    url='${loki-server:caucase-url}',
    data_dir='${directory:caucase-updater-loki-fluent-bit-client}',
    crt_path='${loki-fluent-bit-client-certificate:cert-file}',
    ca_path='${loki-fluent-bit-client-certificate:ca-file}',
    crl_path='${loki-fluent-bit-client-certificate:crl-file}',
    key_path='${loki-fluent-bit-client-certificate:key-file}',
    template_csr='${loki-fluent-bit-client-certificate-prepare-csr:csr}',
    openssl=openssl_bin,
)}}

[fluent-bit]
recipe = slapos.cookbook:wrapper
command-line = ${:nice} {{ fluent_bit_bin }} --config ${fluent-bit-config-file:location}
wrapper-path = ${directory:service}/fluent-bit
hash-files =
  ${fluent-bit-config-file:location}
# TODO: control nice of the agent ?
{% if 0 %}
nice = nice -19 chrt --idle 0 ionice -c3
{% else %}
nice =
{% endif %}

http-port = 19080
ip = ${instance-parameter:ipv4-random}
url = http://${:ip}:${:http-port}

[fluent-bit-config-file]
recipe = slapos.recipe.build
location = ${directory:etc}/${:_buildout_section_name_}.yaml
slapparameter-dict = ${slap-configuration:configuration}
filters-lua = {{ fluent_bit_filters_lua }}
db-ip-city-lite = {{ db_ip_city_lite }}
tail-db-base-path = ${directory:fluent-bit-dir}
depends = ${loki-fluent-bit-client-certificate:recipe}
{% raw %}
init =
  import zc.buildout
  import pkg_resources

  buildout_options = self.buildout["buildout"]
  zc.buildout.easy_install.install(
    ["pytz"],
    dest=None,
    working_set=pkg_resources.working_set,
    path=[
      buildout_options["develop-eggs-directory"],
      buildout_options["eggs-directory"]])

  import datetime
  import itertools
  import json
  import pathlib
  import re

  import pytz

  slapparameter_dict = self.options['slapparameter-dict']
  slap_connection = self.buildout["slap-connection"]
  fluent_bit = self.buildout["fluent-bit"]
  loki_certificate = self.buildout['loki-fluent-bit-client-certificate']
  db_ip_city_lite = self.options['db-ip-city-lite']
  filters_lua = self.options['filters-lua']

  self._config_files = {}  # files to create during install step
  access_path_dict = {}
  inputs = []
  multiline_parsers = []
  parsers = []
  filters = []
  outputs = []
  cfg = {
    "service": {
      "batch_wait": 5,
      "http_server": True,
      "http_listen": fluent_bit['ip'],
      "http_port": fluent_bit['http-port'],
      "health_check": True,

    },
    "multiline_parsers": multiline_parsers,
    "parsers": parsers,
    "pipeline": {
      "inputs": inputs,
      "filters": filters,
      "outputs": outputs,
    },
  }

  def _add_parser(parser):
    if parser['name'] not in {p['name'] for p in parsers}:
      parsers.append(parser)

  def _add_multiline_parser(parser):
    if parser['name'] not in {p['name'] for p in multiline_parsers}:
      multiline_parsers.append(parser)

  def _add_filter(filter_):
    if (filter_['name'], filter_['match']) not in {(f['name'], f['match']) for f in filters}:
      filters.append(filter_)

  def _safe_filename(name):
    return re.sub(r'[^a-zA-Z0-9._-]', '_', name)

  def _safe_label(name):
    if " " in name:
      return f'"{name}"'
    return name

  def _utc_time_offset_from_timezone(timezone):
    # Timezone can only be specified as an offset from default time zone, see
    # https://github.com/fluent/fluent-bit/issues/10331
    # Our work-around approach  is to compute the offset during config file generation,
    # in `slapos node instance` step. For timezone with daylight saving (such as Europe/Paris)
    # this is not good, because `slapos node instance` runs once a day.
    try:
      tz = pytz.timezone(timezone)
    except pytz.exceptions.UnknownTimeZoneError as e:
      print(f"Ignoring unknown timezone {timezone} {e}")
      return timezone
    offset_seconds = int(tz.utcoffset(datetime.datetime.utcnow()).total_seconds())
    sign = "+" if offset_seconds >= 0 else "-"
    offset_seconds = abs(offset_seconds)
    hours = offset_seconds // 3600
    minutes = (offset_seconds % 3600) // 60
    return f"{sign}{hours:02d}{minutes:02d}"

  def _add_input_filter_tags(tag, log_type, application, partition):
    added_keys = [
      f"app {_safe_label(application['name'])}",
      f"name {_safe_label(partition['name'])}",
      f"log_type {_safe_label(log_type)}",
    ]
    if partition.get('reference'):
      added_keys.append(f"partition_reference {_safe_label(partition['reference'])}")
    for k, v in partition.get('static-tags', {}).items():
      added_keys.append(f"field_{k} {_safe_label(v)}")
    _add_filter(
      {
        "name": "modify",
        "match": tag,
        "Add": added_keys
      },
    )

  def _add_tail_input(tag:str, log_type:str, application:dict, partition:dict, file_pattern:str, parser=None, multiline_parser=None, buffer_max_size=None):
    db_path = str(pathlib.Path(self.options['tail-db-base-path']) / _safe_filename(tag)) + '.db'

    partition_root_directory = ''
    if partition.get('reference') and 'instance-root' in application:
      instance_root = pathlib.Path(application['instance-root'])
      partition_root_directory = instance_root / partition['reference']
      file_pattern = file_pattern.format(partition_root_directory=partition_root_directory)

    inpt = {
      "name": "tail",
      "path": file_pattern,
      "path_key": "filename",
      "tag": tag,
      "db": db_path,
      "skip_long_lines": "on",
    }
    if parser:
      inpt['parser'] = parser
    if multiline_parser:
      inpt['multiline.parser'] = multiline_parser
    if buffer_max_size:
      inpt['buffer_max_size'] = buffer_max_size
    access_path_dict[file_pattern] = "r"
    inputs.append(inpt)
    _add_input_filter_tags(tag, log_type, application, partition)

  def _add_systemd_input(tag:str, application:dict, partition:dict, systemd_filter:str):
    db_path = str(pathlib.Path(self.options['tail-db-base-path']) / _safe_filename(tag)) + '.db'

    inpt = {
      "name": "systemd",
      "systemd_filter": systemd_filter,
      "tag": tag,
      "db": db_path,
    }
    access_path_dict["/var/log/journal/*/*"] = "r"
    inputs.append(inpt)
    _add_input_filter_tags(tag, "systemd", application, partition)

  def add_erp5_balancer(application, partition):
    _add_parser(
      {
        "name": "parser.erp5.balancer.access_log",
        "format": "regex",
        "regex": r'^(?<client_ip>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>\S+))?(?: +(?<protocol>\S+))?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^"]*)" "(?<agent>[^"]*)\")?(\s|)(?<duration>[\d]*)?$',
        "time_key": "time",
        "time_format": "%d/%b/%Y:%H:%M:%S %z",
        "types": "size:integer, duration:integer",
      }
    )
    _add_filter(
      {
        "name": "geoip2",
        "match": "logs.erp5.balancer.access.*",
        "database": db_ip_city_lite,
        "lookup_key": "client_ip",
        "record": [
            "country client_ip %{country.names.en}",
            "country_isocode client_ip %{country.iso_code}",
            "city client_ip %{city.names.en}",
            "latitude client_ip %{location.latitude}",
            "longitude client_ip %{location.longitude}",
        ],
      },
    )
    _add_tail_input(
      f"logs.erp5.balancer.access.{application['name']}-{partition['name']}",
      "erp5.balancer.access",
      application,
      partition,
      "{partition_root_directory}/var/log/apache-access.log",
      parser="parser.erp5.balancer.access_log",
    )
    _add_tail_input(
      f"logs.erp5.balancer.error.{application['name']}-{partition['name']}",
      "erp5.balancer.access",
      application,
      partition,
      "{partition_root_directory}/var/log/apache-error.log",
    )

  def add_erp5_zope(application, partition):
    timezone = partition.get('timezone', 'UTC')
    _add_multiline_parser(
      {
        "name": "multiline_parser.erp5.zope.event_log",
        "type": "regex",
        "flush_timeout": 5000,
        "rules": [
          {
            "state": "start_state",
            "regex": "^------\n",
            "next_state": "first_line"
          },
          {
            "state": "first_line",
            "regex": r"\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3} \S+ \S+ .*",
            "next_state": "continuation"
          },
          {
            "state": "continuation",
            "regex": "^[^-]+",
            "next_state": "continuation"
          }
        ]
      }
    )
    _add_parser(
      {
        "name": f"parser.erp5.zope.event.{timezone}",
        "format": "regex",
        "regex": r"^------\n(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3}) (?<level>\S+) .*",
        "time_key": "timestamp",
        "time_format": "%Y-%m-%d %H:%M:%S,%L",
        "time_offset": _utc_time_offset_from_timezone(timezone)
      }
    )
    _add_filter(
      {
        "name": "parser",
        "match": f"logs.erp5.zope.event.{timezone}.*",
        "parser": f"parser.erp5.zope.event.{timezone}",
        "key_name": "log",
        "preserve_key": True,
        "reserve_data": True,
      },
    )
    _add_tail_input(
      f"logs.erp5.zope.event.{timezone}.{application['name']}-{partition['name']}",
      "erp5.zope.event",
      application,
      partition,
      "{partition_root_directory}/var/log/zope-*-event.log",
      multiline_parser="multiline_parser.erp5.zope.event_log",
      buffer_max_size="512k",
    )

  def add_erp5_zope_front(application, partition):
    _add_parser(
      {
        "name": "parser.erp5.zope.access_log",
        "format": "regex",
        "regex": r'^(?<client_ip>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>\S+))?(?: +(?<protocol>\S+))?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^"]*)" "(?<agent>[^"]*)")?(\s|)$',
        "time_key": "time",
        "time_format": "%d/%b/%Y:%H:%M:%S %z",
        "types": "size:integer",
      }
    )
    # exclude haproxy health checks
    _add_filter(
      {
        "name": "grep",
        "match": "logs.erp5.zope.access.*",
        "logical_op": "and",
        "exclude": [
          "path ^/$",
          "method ^GET$",
          "agent ^-$",
        ]
      }
    )
    _add_filter(
      {
        "name": "geoip2",
        "match": "logs.erp5.zope.access.*",
        "database": db_ip_city_lite,
        "lookup_key": "client_ip",
        "record": [
            "country client_ip %{country.names.en}",
            "country_isocode client_ip %{country.iso_code}",
            "city client_ip %{city.names.en}",
            "latitude client_ip %{location.latitude}",
            "longitude client_ip %{location.longitude}",
        ],
      },
    )
    _add_tail_input(
      f"logs.erp5.zope.access.{application['name']}-{partition['name']}",
      "erp5.zope.access",
      application,
      partition,
      "{partition_root_directory}/var/log/zope-*-Z2.log",
      parser="parser.erp5.zope.access_log"
    )

    timezone = partition.get('timezone', 'UTC')
    _add_multiline_parser(
      {
        "name": "multiline_parser.erp5.zope.long_request",
        "type": "regex",
        "flush_timeout": 5000,
        "rules": [
          {
            "state": "start_state",
            "regex": r"^\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3} - Thread \d+: Started on \d+.*",
            "next_state": "continuation"
          },
          {
            "state": "continuation",
            "regex": "^([^#].*|)$",
            "next_state": "continuation"
          }
        ]
      }
    )
    _add_parser(
      {
        "name": f"parser.erp5.zope.long_request.{timezone}",
        "format": "regex",
        "regex": r"^(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3}) - Thread \d+: Started on \d+.*",
        "time_key": "timestamp",
        "time_format": "%Y-%m-%d %H:%M:%S,%L",
        "time_offset": _utc_time_offset_from_timezone(timezone)
      }
    )
    _add_filter(
      {
        "name": "parser",
        "match": f"logs.erp5.zope.long_request.{timezone}.*",
        "parser": f"parser.erp5.zope.long_request.{timezone}",
        "key_name": "log",
        "preserve_key": True,
        "reserve_data": True,
      },
    )
    _add_tail_input(
      f"logs.erp5.zope.long_request.{timezone}.{application['name']}-{partition['name']}",
      "erp5.zope.long_request",
      application,
      partition,
      "{partition_root_directory}/var/log/longrequest_logger_zope-*.log",
      multiline_parser="multiline_parser.erp5.zope.long_request",
      buffer_max_size="512k",
    )

  def add_erp5_zeo(application, partition):
    _add_multiline_parser(
      {
        "name": "multiline_parser.erp5.zeo.event_log",
        "type": "regex",
        "flush_timeout": 5000,
        "rules": [
          {
            "state": "start_state",
            "regex": "^------\n",
            "next_state": "first_line"
          },
          {
            "state": "first_line",
            "regex": r"\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3} \S+ \S+ .*",
            "next_state": "continuation"
          },
          {
            "state": "continuation",
            "regex": "^[^-]+",
            "next_state": "continuation"
          }
        ]
      }
    )
    _add_parser(
      {
        "name": "parser.erp5.zeo.event",
        "format": "regex",
        "regex": r"^------\n(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{1,2}:\d{2}:\d{2},\d{3}) (?<level>\S+) .*",
        "time_key": "timestamp",
        "time_format": "%Y-%m-%d %H:%M:%S,%L",
      }
    )
    _add_filter(
      {
        "name": "parser",
        "match": "logs.erp5.zeo.event.*",
        "parser": "parser.erp5.zeo.event",
        "key_name": "log",
        "preserve_key": True,
        "reserve_data": True,
      },
    )
    _add_tail_input(
      f"logs.erp5.zeo.event.{application['name']}-{partition['name']}",
      "erp5.zeo.event",
      application,
      partition,
      "{partition_root_directory}/var/log/zeo-*.log",
      multiline_parser="multiline_parser.erp5.zeo.event_log",
    )

  def add_mariadb(application, partition):
    _add_parser(
      {
        "name": "parser.mariadb.slow_log",
        "format": "regex",
        # we capture `query` for the lua filter, we drop trailing ; if any to have same hashes as pt-query-digest
        "regex": ".*Query_time: (?<field_query_time>[\d\\.]+).*\n.*\nSET timestamp=(?<timestamp>\\d+);\n(?<intermediate_query>(?:.*\n?)*)[;]*",
        "time_key": "timestamp",
        "time_format": "%s",
        "types": "field_query_time:float",
      }
    )
    _add_multiline_parser(
      {
        "name": "multiline_parser.mariadb.slow_log",
        "type": "regex",
        "flush_timeout": 5000,
        "rules": [
          {
            "state": "start_state",
            "regex": "^# User@Host: .*",
            "next_state": "thread_id"
          },
          {
            "state": "thread_id",
            "regex": "^# Thread_id: (\d+).*",
            "next_state": "query_time"
          },
          {
            "state": "query_time",
            "regex": "^# Query_time: .*",
            "next_state": "rows_affected"
          },
          {
            "state": "rows_affected",
            "regex": "^# Rows_affected: .*",
            "next_state": "timestamp"
          },
          {
            "state": "timestamp",
            "regex": "^SET timestamp=\d+;",
            "next_state": "continuation"
          },
          {
            "state": "continuation",
            "regex": "^([^#].*|)$",
            "next_state": "continuation"
          }
        ]
      }
    )
    # every second, a line like `# Time: 250608 18:47:19` is added to
    # the slow log, we exclude it with this `grep`
    _add_filter(
      {
        "name": "grep",
        "match": "logs.mariadb.slow_log.*",
        "exclude": "$log ^# Time: ",
      }
    )
    _add_filter(
      {
        "name": "parser",
        "match": "logs.mariadb.slow_log.*",
        "parser": "parser.mariadb.slow_log",
        "key_name": "log",
        "preserve_key": True,
        "reserve_data": True,
      }
    )
    _add_filter(
      {
        "name": "lua",
        "match": "logs.mariadb.slow_log.*",
        "script": filters_lua,
        "call": "query_fingerprint_filter"
      }
    )
    _add_tail_input(
      f"logs.mariadb.slow_log.{application['name']}-{partition['name']}",
      "mariadb.slow_log",
      application,
      partition,
      "{partition_root_directory}/var/log/mariadb_slowquery.log",
      multiline_parser='multiline_parser.mariadb.slow_log',
      buffer_max_size="512k",
    )
    _add_tail_input(
      f"logs.mariadb.error.{application['name']}-{partition['name']}",
      "mariadb.error",
      application,
      partition,
      "{partition_root_directory}/var/log/mariadb_error.log",
    )

  def _add_default_filters():
    filters.append(
      {
        "name": "nest",
        "match": "logs.*",
        "operation": "nest",
        "wildcard": "field_*",
        "nest_under": "fields",
        "remove_prefix": "field_",
      }
    )
    # at the end, remove intermediate fields used internally in the pipeline
    filters.append(
      {
        "name": "modify",
        "match": "logs.*",
        "Remove_Wildcard": ["intermediate_"],
      }
    )
    # remove the -----\n from zope and zeo event logs, at the end, after the
    # parsing
    for match in "logs.erp5.zeo.event.*", "logs.erp5.zope.event.*":
      _add_filter(
        {
          "name": "lua",
          "match": match,
          "call": "remove_prefix",
          "code": r"""
            function remove_prefix(tag, timestamp, record)
              record["log"] = string.gsub(record["log"], "^%-%-%-%-%-%-\n", "")
              return 1, timestamp, record
            end
          """,
        }
      )

  def _add_outputs():
    outputs.append(
      {
        "name": "loki",
        "match": "logs.*",
        "host": self.buildout['loki-server']['hostname'],
        "port": self.buildout['loki-server']['port'],
        "labels": f"computer_id={_safe_label(slap_connection['computer-id'])}, hostname=$" + "{HOSTNAME}",
        "tls": True,
        "tls.ca_file": loki_certificate['ca-file'],
        "tls.crt_file": loki_certificate['cert-file'],
        "tls.key_file": loki_certificate['key-file'],
        "label_keys": "$app, $filename, $level, $log_type, $name, $partition_reference",
        "remove_keys": "$app, $fields, $filename, $level, $log_type, $partition_reference",
        "drop_single_key": True,
        "line_format": "key_value",
        "structured_metadata_map_keys": "$fields"
      }
    )

  for application in slapparameter_dict.get('applications', []):
    for partition in application.get('partitions', []):
      partition.setdefault("type", "default")
      if partition['type'] == 'erp5/balancer':
        add_erp5_balancer(application, partition)
      elif partition['type'] in ('erp5/zope-activity', 'erp5/zope-front'):
        add_erp5_zope(application, partition)
        if partition['type'] == 'erp5/zope-front':
          add_erp5_zope_front(application, partition)
      elif partition['type'] in ('erp5/mariadb', 'mariadb'):
        add_mariadb(application, partition)
      elif partition['type'] == 'erp5/zeo':
        add_erp5_zeo(application, partition)
      for log_file_pattern in partition.get('log-file-patterns', []):
        _add_tail_input(
          f"logs.log-file-patterns-{application['name']}-{partition['name']}",
          "log",
          application,
          partition,
          log_file_pattern,
        )
      if partition.get('systemd-filter'):
        tag = f"logs.systemd-{application['name']}-{partition['name']}"
        _add_systemd_input(
          f"logs.log-file-patterns-{application['name']}-{partition['name']}",
          application,
          partition,
          partition['systemd-filter'],
        )

  _add_default_filters()
  _add_outputs()

  self._config_files[options['location']] = json.dumps(cfg, indent=2)
  options['access-path-dict'] = access_path_dict

install =
  for fname, content in self._config_files.items():
    with open(fname, 'w') as f:
      f.write(content)
{% endraw %}

[fluent-bit-listen-promise]
<= check-url-available-promise
url = http://${fluent-bit:ip}:${fluent-bit:http-port}/api/v1/health

[telegraf-listen-promise]
recipe = slapos.cookbook:wrapper
command-line =
  test -S ${telegraf-config-file:input-socket}
wrapper-path = ${directory:promise}/${:_buildout_section_name_}


[facl-script]
recipe = slapos.recipe.build
fluent-bit-access-path-dict = ${fluent-bit-config-file:access-path-dict}
telegraf-access-path-dict = ${telegraf-config-file:access-path-dict}
install =
  import itertools
  import os
  import pathlib
  import pwd
  import shlex

  user = pwd.getpwuid(os.getuid()).pw_name
  script_code = ''

  def quote_path(p):
    # quote, but preserve *
    p = str(p)
    assert '__STAR__' not in p
    p = p.replace('*', '__STAR__')
    p = shlex.quote(p)
    p = p.replace('__STAR__', '*')
    return p

  # make sure we can access the parents folders
  parent_access = {}
  def check_parent_access(path):
    parent = path.parent
    if parent != path:
      parent_access[str(parent)] = 'x'
      check_parent_access(parent)
  for path_spec, access in itertools.chain(
      options['fluent-bit-access-path-dict'].items(),
      options['telegraf-access-path-dict'].items()):
    path = pathlib.Path(path_spec)
    check_parent_access(path)

  for path_spec, access in sorted(itertools.chain(
      options['fluent-bit-access-path-dict'].items(),
      options['telegraf-access-path-dict'].items(),
      parent_access.items())):
    path = pathlib.Path(path_spec)
    if '*' in path_spec:
      script_code += f'setfacl --modify=u:{user}:rx {quote_path(path.parent)}\n'
    script_code += f'setfacl --modify=u:{user}:{access} {quote_path(path)}\n'
  pathlib.Path(location).write_text(script_code)

[promises]
recipe =
instance-promises =
  ${fluent-bit-listen-promise:path}
  ${telegraf-listen-promise:wrapper-path}

[publish-connection-parameter]
recipe = slapos.cookbook:publish.serialised
telegraf-extra-config-dir = ${telegraf:extra-config-dir}
facl-script = ${facl-script:location}
fluent-bit-url = ${fluent-bit:url}
